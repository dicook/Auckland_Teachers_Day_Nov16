---
title: "The Role of Open Data, Open Source Software and Data Visualisation in Developing Quantitative Citizenship"
author: "Professor Di Cook, Econometrics and Business Statistics"
date: "Auckland  Statistics Teachers' Day, Nov 25, 2016"
output:
  beamer_presentation: 
    theme: Monash
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      error = FALSE)
```

## Power to the People

H. G. Wells (1903) Mankind in the Making

*"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write!"*

## Open data, open source

- Data is available everywhere today, publicly, free
- Software, very powerful software, for analysis of data is available publicly, free
- Combined with a knowledge of mathematics and statistics empowers each of us to contribute to understand and improve our world
-
- I'm going to show you a few projects that I worked on using open data and open software, and how it helped me understand the world a little better: gender gap, climate change and politics. Oh, and where people walk in Melbourne city, too.

## Math Gender Gap

\centerline{\includegraphics[width=6in]{gendergap.pdf}}

## Education: OECD PISA

- OECD PISA survey ``the world's global metric for quality, equity and efficiency in school education".
- Workforce readiness of 15-year old students
- 500,000 students were tested across 65 countries and 18,000 schools
- Math, reading and science
- Data available from [http://www.oecd.org/pisa/keyfindings/pisa-2012-results.htm](http://www.oecd.org/pisa/keyfindings/pisa-2012-results.htm)


## Procedure

Measuring the difference

1. Compute the mean by country and gender
2. Difference the means: boys - girls

## Confidence intervals

Bootstrap is used to produce the confidence intervals

1. Sample the sample, with replacement, separately by country
2. Compute the difference
3. Repeat 1 and 2, 1000 times
4. Keep the 2.5 and 97.5 percentiles, to yield 95% confidence interval

---

```{r load_packages, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE, results='hide'}
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(ggmap)
library(rworldmap)
library(grid)    
library(scales)
library(gridExtra)
library(purrr)
library(boot)
library(readr)
```

```{r load_data, echo = FALSE, message = FALSE, warning = FALSE, results='hide', cache=FALSE}
student2012.sub <- readRDS("student_sub.rds")
```

```{r mapdata, echo = FALSE, message = FALSE, warning = FALSE, results='hide', cache=FALSE}
world <- getMap(resolution = "low")
extractPolys <- function(p) {
  polys <- NULL
  for (i in 1:length(p)) {
    for (j in 1:length(p[[i]]@Polygons)) {
      x <- p[[i]]@Polygons[[j]]@coords
      polys$lon <- c(polys$lon, x[,1])
      polys$lat <- c(polys$lat, x[,2])
      polys$ID <- c(polys$ID, rep(p[[i]]@ID, nrow(x)))
      polys$region <- c(polys$region, rep(paste(p[[i]]@ID, j, sep="_"), nrow(x)))
      polys$order <- c(polys$order, 1:nrow(x))
    }
  }
  return(data.frame(polys))
}
polys <- extractPolys(world@polygons)

# Map theme
theme_map <- theme_bw()
theme_map$line <- element_blank()
theme_map$strip.text <- element_blank()
theme_map$axis.text <- element_blank()
theme_map$plot.title <- element_blank()
theme_map$axis.title <- element_blank()
theme_map$panel.border <- element_rect(colour = "grey90", size=1, fill=NA)
```

```{r dataprep, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE}
student2012.sub$ST04Q01 <- factor(student2012.sub$ST04Q01, 
  levels=c(1,2), labels=c("Female", "Male"))
```

```{r computemean, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE, error=FALSE, fig.width=6, fig.height=7, fig.align='center'}
# Calculate the statistics
student2012.stats <- student2012.sub %>% 
  group_by(CNT) %>%
  summarise(mathgap=mean(PV1MATH[ST04Q01=="Male"], na.rm=T)-
                    mean(PV1MATH[ST04Q01=="Female"], na.rm=T),
            wmathgap=weighted.mean(PV1MATH[ST04Q01=="Male"], 
                                   w=SENWGT_STU[ST04Q01=="Male"], na.rm=T)-
                     weighted.mean(PV1MATH[ST04Q01=="Female"],
                                   w=SENWGT_STU[ST04Q01=="Female"], na.rm=T))

# Compute confidence intervals
cifn <- function(d, i) {
  x <- d[i,]
  ci <- weighted.mean(x$PV1MATH[x$ST04Q01=="Male"], 
                                   w=x$SENWGT_STU[x$ST04Q01=="Male"], na.rm=T)-
                     weighted.mean(x$PV1MATH[x$ST04Q01=="Female"],
                                   w=x$SENWGT_STU[x$ST04Q01=="Female"], na.rm=T)
  ci
}
bootfn <- function(d) {
  r <- boot(d, statistic=cifn, R=100)
  l <- sort(r$t)[5]
  u <- sort(r$t)[95]
  ci <- c(l, u)
  return(ci)
}
#student2012.sub.summary.gap.boot <- ddply(student2012.sub, .(CNT), bootfn)
student2012.sub.summary.gap.boot <- student2012.sub %>% 
  split(.$CNT) %>% purrr::map(bootfn) %>% data.frame() %>%
  gather(CNT, value)
student2012.sub.summary.gap.boot$ci <- 
  rep(c("ml","mu"), length(unique(student2012.sub.summary.gap.boot$CNT)))
student2012.sub.summary.gap.boot.wide <- student2012.sub.summary.gap.boot %>% spread(ci, value)
student2012.sub.summary.gap <- merge(student2012.stats, student2012.sub.summary.gap.boot.wide)

# Match three digit codes to country names 
student2012.sub.summary.gap$name <- NA
for (i in 1:length(student2012.sub.summary.gap$name))  
  student2012.sub.summary.gap$name[i] <-
  isoToName(as.character(student2012.sub.summary.gap$CNT[i]))
# QCN is Shanghai, not whole of China - Don't know what country TAP is
student2012.sub.summary.gap$name[student2012.sub.summary.gap$CNT == "QCN"] <- isoToName("CHN")
student2012.sub.summary.gap$name[student2012.sub.summary.gap$CNT == "TAP"] <- "TAP"

# Make a categorical gap variable
#student2012.sub.summary.gap <-  student2012.sub.summary.gap %>% 
#  mutate(wmathgap_cat = cut(wmathgap, breaks=c(-10,-5, 5, 30), 
#                            labels=c("girls", "same", "boys")))
student2012.sub.summary.gap$wmathgap_cat <- "same"
student2012.sub.summary.gap$wmathgap_cat[student2012.sub.summary.gap$ml > 0] <- "boys"
student2012.sub.summary.gap$wmathgap_cat[student2012.sub.summary.gap$mu < 0] <- "girls"

# Set order of countries by math gap
student2012.sub.summary.gap$CNT <- factor(student2012.sub.summary.gap$CNT, 
      levels=student2012.sub.summary.gap$CNT[order(student2012.sub.summary.gap$wmathgap)])
student2012.sub.summary.gap$name <- factor(student2012.sub.summary.gap$name, 
      levels=student2012.sub.summary.gap$name[order(student2012.sub.summary.gap$wmathgap)])

# Plot
ggplot(data=student2012.sub.summary.gap) + 
  geom_hline(yintercept=0, colour="grey80") + coord_flip() + theme_bw() + 
  geom_point(aes(x=name, y=wmathgap, color=wmathgap_cat), size=3) + 
  geom_segment(aes(x=name, xend=name, y=ml, yend=mu, color=wmathgap_cat)) + 
  xlab("") +  
  scale_colour_manual("", values=c("boys"="skyblue", "girls"="pink", "same"="lightgreen")) +
  scale_y_continuous("Girls <----------> Boys", breaks=seq(-30, 30, 10), limits=c(-35, 35), 
                     labels=c(seq(30, 0, -10), seq(10, 30, 10))) + 
  theme(axis.text.x = element_text(size=5), axis.text.y = element_text(size=5), 
        axis.title = element_text(size=7), legend.text = element_text(size=5),
        legend.title = element_text(size=5))
```

---

```{r maps, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE, fig.width=8, fig.height=4}
polys <- polys %>% rename(name = ID)
student2012.sub.map <- left_join(student2012.sub.summary.gap, polys)
student2012.sub.map <- student2012.sub.map %>% arrange(region, order)

ggplot(data=polys) + 
  geom_path(aes(x=lon, y=lat, group=region, order=order), colour=I("grey90"), size=0.1) + 
  geom_polygon(data=student2012.sub.map, aes(x=lon, y=lat, group=region, order=order,  fill=wmathgap_cat)) +
  scale_fill_manual("Diff>5", values=c("boys"="skyblue", "girls"="pink", "same"="lightgreen")) + 
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0)) +
  coord_equal() + theme_map + theme(legend.position="None")
```

## Reading Gap

```{r computereadmean, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE, error=FALSE, fig.width=6, fig.height=7, fig.align='center'}
# Calculate the statistics
student2012.stats <- student2012.sub %>% 
  group_by(CNT) %>%
  summarise(readgap=mean(PV1READ[ST04Q01=="Male"], na.rm=T)-
                    mean(PV1READ[ST04Q01=="Female"], na.rm=T),
            wreadgap=weighted.mean(PV1READ[ST04Q01=="Male"], 
                                   w=SENWGT_STU[ST04Q01=="Male"], na.rm=T)-
                     weighted.mean(PV1READ[ST04Q01=="Female"],
                                   w=SENWGT_STU[ST04Q01=="Female"], na.rm=T))

# Compute confidence intervals
cifn <- function(d, i) {
  x <- d[i,]
  ci <- weighted.mean(x$PV1READ[x$ST04Q01=="Male"], 
                                   w=x$SENWGT_STU[x$ST04Q01=="Male"], na.rm=T)-
                     weighted.mean(x$PV1READ[x$ST04Q01=="Female"],
                                   w=x$SENWGT_STU[x$ST04Q01=="Female"], na.rm=T)
  ci
}
bootfn <- function(d) {
  r <- boot(d, statistic=cifn, R=100)
  l <- sort(r$t)[5]
  u <- sort(r$t)[95]
  ci <- c(l, u)
  return(ci)
}
#student2012.sub.summary.gap.boot <- ddply(student2012.sub, .(CNT), bootfn)
student2012.sub.summary.gap.boot <- student2012.sub %>% 
  split(.$CNT) %>% purrr::map(bootfn) %>% data.frame() %>%
  gather(CNT, value)
student2012.sub.summary.gap.boot$ci <- 
  rep(c("ml","mu"), length(unique(student2012.sub.summary.gap.boot$CNT)))
student2012.sub.summary.gap.boot.wide <- student2012.sub.summary.gap.boot %>% spread(ci, value)
student2012.sub.summary.gap <- merge(student2012.stats, student2012.sub.summary.gap.boot.wide)

# Match three digit codes to country names 
student2012.sub.summary.gap$name <- NA
for (i in 1:length(student2012.sub.summary.gap$name))  
  student2012.sub.summary.gap$name[i] <-
  isoToName(as.character(student2012.sub.summary.gap$CNT[i]))
# QCN is Shanghai, not whole of China - Don't know what country TAP is
student2012.sub.summary.gap$name[student2012.sub.summary.gap$CNT == "QCN"] <- isoToName("CHN")
student2012.sub.summary.gap$name[student2012.sub.summary.gap$CNT == "TAP"] <- "TAP"

# Make a categorical gap variable
#student2012.sub.summary.gap <-  student2012.sub.summary.gap %>% 
#  mutate(wreadgap_cat = cut(wreadgap, breaks=c(-10,-5, 5, 30), 
#                            labels=c("girls", "same", "boys")))
student2012.sub.summary.gap$wreadgap_cat <- "same"
student2012.sub.summary.gap$wreadgap_cat[student2012.sub.summary.gap$ml > 0] <- "boys"
student2012.sub.summary.gap$wreadgap_cat[student2012.sub.summary.gap$mu < 0] <- "girls"

# Set order of countries by read gap
student2012.sub.summary.gap$CNT <- factor(student2012.sub.summary.gap$CNT, 
      levels=student2012.sub.summary.gap$CNT[order(student2012.sub.summary.gap$wreadgap)])
student2012.sub.summary.gap$name <- factor(student2012.sub.summary.gap$name, 
      levels=student2012.sub.summary.gap$name[order(student2012.sub.summary.gap$wreadgap)])

# Plot
ggplot(data=student2012.sub.summary.gap) + 
  geom_hline(yintercept=0, colour="grey80") + coord_flip() + theme_bw() + 
  geom_point(aes(x=name, y=wreadgap, color=wreadgap_cat), size=3) + 
  geom_segment(aes(x=name, xend=name, y=ml, yend=mu, color=wreadgap_cat)) + 
  xlab("") +  
  scale_colour_manual("", values=c("boys"="skyblue", "girls"="pink", "same"="lightgreen")) +
  scale_y_continuous("Girls <----------> Boys", breaks=seq(-70, 10, 10), limits=c(-75, 15), 
                     labels=c(seq(70, 0, -10), 10)) + 
  theme(axis.text.x = element_text(size=5), axis.text.y = element_text(size=5), 
        axis.title = element_text(size=7), legend.text = element_text(size=5),
        legend.title = element_text(size=5))
```

---

```{r mapsread, cache=FALSE, echo = FALSE, message = FALSE, warning = FALSE, fig.width=8, fig.height=4}
student2012.sub.map <- left_join(student2012.sub.summary.gap, polys)
student2012.sub.map <- student2012.sub.map %>% arrange(region, order)

ggplot(data=polys) + 
  geom_path(aes(x=lon, y=lat, group=region, order=order), colour=I("grey90"), size=0.1) + 
  geom_polygon(data=student2012.sub.map, aes(x=lon, y=lat, group=region, order=order,  fill=wreadgap_cat)) +
  scale_fill_manual("Diff>5", values=c("boys"="skyblue", "girls"="pink", "same"="lightgreen")) + 
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0)) +
  coord_equal() + theme_map + theme(legend.position="None")
```


## Climate change: What is it about carbon dioxide?

\centerline{\includegraphics[width=6in]{carbon.pdf}}

---

- "Scientific consensus states that carbon emissions must be reduced by 80% by 2050 to avoid temperature rise of more than 2$^o$C." [Carbon Neutral](http://www.carbonneutral.com/resource-hub/carbon-offsetting-explained)
- Carbon offsets: Carbon offsetting is the use of carbon credits to enable businesses to compensate for their emissions.
- Kyoto protocol in 1992, attempt to get international cooperation to reduce emissions. 

## Carbon dioxide data

- Data is collected at a number of locations world wide. 
- See [Scripps Inst. of Oceanography](http://scrippsco2.ucsd.edu/data/atmospheric_co2) 
- Let's pull the data from the web and take a look ...
- 
- Recordings from South Pole (SPO), Kermadec Islands (KER), Mauna Loa Hawaii (MLO), La Jolla Pier, California (LJO), Point Barrow, Alaska (PTB).

---

```{r CO2, fig.width=10, fig.height=5, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE}
CO2.ptb<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ptb.csv", sep=",", skip=69)
colnames(CO2.ptb)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ptb$lat<-71.3
CO2.ptb$lon<-(-156.6)
CO2.ptb$stn<-"ptb"

CO2.ljo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ljo.csv", sep=",", skip=69)
colnames(CO2.ljo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ljo$lat<-32.9
CO2.ljo$lon<-(-117.3)
CO2.ljo$stn<-"ljo"

CO2.mlo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_mlo.csv", sep=",", skip=69)
colnames(CO2.mlo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.mlo$lat<-19.5
CO2.mlo$lon<-(-155.6)
CO2.mlo$stn<-"mlo"

CO2.spo<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_spo.csv", sep=",", skip=69)
colnames(CO2.spo)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.spo$lat<- (-90.0)
CO2.spo$lon<-0
CO2.spo$stn<-"spo"

CO2.ker<-read.table("http://scrippsco2.ucsd.edu/sites/default/files/data/flask_co2_and_isotopic/daily_co2/fldav_ker.csv", sep=",", skip=69)
colnames(CO2.ker)<-c("date", "time", "day", "decdate", "n", "flg", "co2")
CO2.ker$lat<-(-29.2)
CO2.ker$lon<-(-177.9)
CO2.ker$stn<-"ker"

CO2.all<-rbind(CO2.ker,CO2.ljo,CO2.mlo,CO2.ptb,CO2.spo)
CO2.all$date<-as.Date(CO2.all$date)

CO2.all$invlat=-1*CO2.all$lat
CO2.all$stn=reorder(CO2.all$stn,CO2.all$invlat)

CO2.all.loc <- rbind(CO2.ker[1,],CO2.ljo[1,],CO2.mlo[1,],CO2.ptb[1,],CO2.spo[1,])

p1 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
		facet_wrap(~stn, ncol=1) + theme(axis.text.y=element_text(size = 6), legend.position="none")
p2 <- qplot(date, co2, data=subset(CO2.all, flg < 2), colour=stn, geom="line",xlab="Year",ylab="CO2 (ppm)") + 
  theme(axis.text.y=element_text(size = 6), legend.position="none")
grid.arrange(p1, p2, ncol=2)
```

---

```{r CO2-map, fig.width=4.5, fig.height=2.5, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE, fig.align='center'}
world <- map_data("world")
worldmap <- ggplot(world, aes(x=long, y=lat, group=group)) +
  geom_path(color="grey80", size=0.5) + xlab("") + ylab("") +
  scale_y_continuous(breaks=(-2:2) * 30) +
  scale_x_continuous(breaks=(-4:4) * 45) +
  theme_bw() + theme(aspect.ratio=0.6)
worldmap + geom_point(data=CO2.all.loc, aes(x=lon, y=lat, group=1), colour="red", 
                      size=2, alpha=0) +
  geom_text(data=CO2.all.loc, aes(x=lon, y=lat, label=stn, group=1), 
            colour="orange", size=5)
```

---

- CO$_2$ is increasing, and it looks like it is exponential increase. **I really expected that the concentration would have flattened out with all of the efforts to reduce carbon emissions.**
- The same trend is seen at every location - physical mixxing of our atmosphere. **I was suspicious of the data on first seeing this because it looks too perfect.**
- Some stations show seasonal pattern - actually the more north the more seasonality - population centres and types of trees.

## US politics - 2016 election

- US election process is complicated - depends on the electoral votes for each state in a winner takes all approach
- Polls released on a regular basis
- [Monitoring the Election Visually](http://chance.amstat.org/files/2010/12/Visually.pdf)
- [Can You Buy a President? Politics After the Tillman Act](http://chance.amstat.org/2014/02/president/)

---

```{r fig.align='center'}
polls <- read_csv("polls.csv")
polls <- polls %>% mutate(
    Poll = reorder(Poll, Poll, length)
  ) 
polls <- polls %>%
  mutate(Poll_big = Poll)
levels(polls$Poll_big)[1:25] <- "Other"
polls <- polls %>% mutate(
    Poll_big = reorder(Poll_big, Poll_big, length)
  ) 
brewer_cols <- RColorBrewer::brewer.pal(9, "Paired") # only palette with nine values without grey
library(nullabor)
results <- data.frame(Date = ymd("2016/11/8"), dif = 1)

ggplot(lineup(null_permute('Poll_big'), polls, n=4), 
       aes(x = End_Date, y = Clinton - Trump)) +
  facet_wrap(~.sample, ncol=2) +
    annotate("text", x = ymd("2016/04/1"), y = 10, label = "Clinton", 
             size = 20, colour = "grey90", hjust=0) +
    annotate("text", x = ymd("2016/04/1"), y = -10, label = "Trump", 
             size = 20, colour = "grey90", hjust=0) +
    geom_hline(yintercept = 0, colour = "grey80") + 
    geom_smooth(se = FALSE, colour = "grey50") +
    geom_point(aes(colour = Poll_big), size = 2.9, alpha=0.7) +
    geom_segment(aes(colour = Poll_big, yend = Clinton - Trump, xend = Start_Date)) +
    theme_bw() + 
    theme(legend.position = "none") + 
    scale_colour_manual("Pollster", values = c(brewer_cols, "grey70"), 
                        guide = guide_legend(nrow = 3)) +
  ylim(c(-15, 15)) +
  ylab("Percentage Point Difference") +
  xlab("") + 
  geom_point(data=results, aes(x=Date, y=dif), 
             shape=3, size=5, colour="black")
```

---

- Many pollsters are not operating objectively
- News stories that lead with a poll result may not be accurately reflecting the potential vote outcome
- This year's polls missed the mark

## GOP Swing

```{r fig.align='center', fig.width=12, fig.height=8}
pres <- read_csv("data2016.csv")
pres_smry <- pres %>% 
  filter(cand_name %in% c("Donald Trump","Hillary Clinton")) %>%
  select(fips, cand_name, pct) %>%
  group_by(fips) %>%
  spread(cand_name, pct) %>%
  summarise(dif = `Hillary Clinton`-`Donald Trump`)

pres_smry <- pres %>% 
  filter(cand_name %in% c("Donald Trump","Hillary Clinton")) %>%
  select(state, fips, cand_name, pct) %>%
  group_by(fips) %>%
  spread(cand_name, pct) %>%
  rename(Dem16 = `Hillary Clinton`, Rep16 = `Donald Trump`) %>%
  mutate(Dem16 = Dem16 * 100, Rep16 = Rep16 * 100)
pres_smry <- subset(pres_smry,
                    !is.na(as.numeric(as.character(pres_smry$fips))))
pres_smry$FIPS.Code<-as.numeric(as.character(pres_smry$fips))

pres_12 <- read.csv("data2012.csv")
pres12_smry <- pres_12[,c(1:4,5,11)]
pres12_smry$Dem <- NA
pres12_smry$GOP <- NA

pres12_smry$Dem[pres_12$Last.name=="Obama"]<-pres_12$Votes[pres_12$Last.name=="Obama"]
pres12_smry$Dem[pres_12$Last.name.1=="Obama"]<-pres_12$Votes.1[pres_12$Last.name.1=="Obama"]
pres12_smry$Dem[pres_12$Last.name.2=="Obama"]<-pres_12$Votes.2[pres_12$Last.name.2=="Obama"]
pres12_smry$Dem[pres_12$Last.name.3=="Obama"]<-pres_12$Votes.3[pres_12$Last.name.3=="Obama"]
pres12_smry$Dem[pres_12$Last.name.4=="Obama"]<-pres_12$Votes.4[pres_12$Last.name.4=="Obama"]

pres12_smry$GOP[pres_12$Last.name=="Romney"]<-pres_12$Votes[pres_12$Last.name=="Romney"]
pres12_smry$GOP[pres_12$Last.name.1=="Romney"]<-pres_12$Votes.1[pres_12$Last.name.1=="Romney"]
pres12_smry$GOP[pres_12$Last.name.2=="Romney"]<-pres_12$Votes.2[pres_12$Last.name.2=="Romney"]
pres12_smry$GOP[pres_12$Last.name.3=="Romney"]<-pres_12$Votes.3[pres_12$Last.name.3=="Romney"]
pres12_smry$GOP[pres_12$Last.name.4=="Romney"]<-pres_12$Votes.4[pres_12$Last.name.4=="Romney"]

states<-subset(pres12_smry, FIPS.Code==0)
pres12_smry<-subset(pres12_smry, FIPS.Code!=0)
pres12_smry<-subset(pres12_smry, !is.na(FIPS.Code))

pres12_smry$Dem<-with(pres12_smry,ave(Dem,FIPS.Code,FUN=sum))
pres12_smry$GOP<-with(pres12_smry,ave(GOP,FIPS.Code,FUN=sum))
pres12_smry$TOTAL.VOTES.CAST<-with(pres12_smry,ave(TOTAL.VOTES.CAST,FIPS.Code,FUN=sum))
pres12_smry<-pres12_smry[!duplicated(pres12_smry$FIPS.Code),]
pres12_smry<-pres12_smry %>% mutate(Dempct = Dem/TOTAL.VOTES.CAST*100, 
                                    Reppct = GOP/TOTAL.VOTES.CAST*100)

elections<-merge(pres12_smry,pres_smry,by="FIPS.Code",suffixes=c("12","16"))
#p <- ggplot(elections, aes(x=Reppct, y=Rep16, 
#    label=County.Name)) + #, colour=state)) +
#  geom_point(alpha=0.5) +
#  theme(aspect.ratio=1) + 
#  geom_abline(intercept=0, slope=1, colour="white") + 
#  coord_equal() + xlab("2012") + ylab("2016") + 
#  ggtitle("GOP") + theme(legend.position="none")
#p
p1 <- ggplot(filter(elections, State.Postal == "WI"), 
            aes(x=Reppct, y=Rep16-Reppct, 
    label=County.Name)) + #, colour=state)) +
  geom_hline(yintercept=0, colour="white", size=2) + 
  geom_point(alpha=0.5) +
  theme(aspect.ratio=1) + 
  coord_equal() + xlab("2012") + ylab("2016-2012") + 
  ggtitle("Wisconsin") + theme(legend.position="none")
p2 <- ggplot(filter(elections, State.Postal == "MI"), 
            aes(x=Reppct, y=Rep16-Reppct, 
    label=County.Name)) + #, colour=state)) +
  geom_hline(yintercept=0, colour="white", size=2) + 
  geom_point(alpha=0.5) +
  theme(aspect.ratio=1) + 
  coord_equal() + xlab("2012") + ylab("2016-2012") + 
  ggtitle("Michigan") + theme(legend.position="none")
p3 <- ggplot(filter(elections, State.Postal == "IA"), 
            aes(x=Reppct, y=Rep16-Reppct, 
    label=County.Name)) + #, colour=state)) +
  geom_hline(yintercept=0, colour="white", size=2) + 
  geom_point(alpha=0.5) +
  theme(aspect.ratio=1) + 
  coord_equal() + xlab("2012") + ylab("2016-2012") + 
  ggtitle("Iowa") + theme(legend.position="none")
p4 <- ggplot(filter(elections, State.Postal == "UT"), 
            aes(x=Reppct, y=Rep16-Reppct, 
    label=County.Name)) + #, colour=state)) +
  geom_hline(yintercept=0, colour="white", size=2) + 
  geom_point(alpha=0.5) +
  theme(aspect.ratio=1) + 
  coord_equal() + xlab("2012") + ylab("2016-2012") + 
  ggtitle("Utah") + theme(legend.position="none")
grid.arrange(p1, p2, p3, p4, ncol=2)
#library(plotly)
#ggplotly(p4)
#\centerline{\includegraphics[width=6in]{elections.pdf}}
```

## Australian politics

- First ever ROpenSci, Brisbane April 2016
- Project [eechidna](https://cran.r-project.org/web/packages/eechidna/index.html)
- [Explore the Australian electorate by voting patterns and demographic makeup](https://vimeo.com/167367369)
- Is the Australian electorate gerrymandered? Using permutation to assess this.


```{r echo=FALSE, fig.width=10, fig.height=4}
library(eechidna)
aec2013 <- aec2013_2cp_electorate %>%
  filter(Elected == "Y")
aec_abs <- merge(aec2013, abs2011, by = "Electorate")
aec_abs$PartyGp <- aec_abs$PartyAb
aec_abs$PartyGp[aec_abs$PartyGp %in% c("LP","LNP","NP","CLP")] <- "Coalition"
aec_abs$PartyGp[aec_abs$PartyGp %in% c("IND","PUP","KAP","GRN")] <- "Other"
ggplot(data=aec_abs, aes(x=Population)) + geom_dotplot(binwidth=2900) +
  facet_wrap(~PartyGp, ncol = 3) + ylab("") + xlab("Population ('000)") +
  scale_x_continuous(breaks=seq(75000, 225000, 25000), labels=seq(75, 225, 25))
```

## Permutation procedure

1. Subset to two major parties, Labor vs Coalition
2. Scramble the party label against electorate
3. Compute absolute value of difference in mean populations
4. Repeat 1-3 10,000 times
5. Plot permuted differences, mark observed data value

P-value is the number of differences greater than observed data divided by number of permutations

---


```{r echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
mad <- function(df, shuffle=TRUE) {
  if (shuffle)
    df$PartyGp <- sample(df$PartyGp)
  df_means <- df %>% group_by(PartyGp) %>%
    summarise(m = mean(Population, na.rm=T))
  return(d = abs(df_means$m[1] - df_means$m[2]))
}
aec_abs_sub <- aec_abs %>% filter(PartyGp != "Other")
aec_abs_meandif <- mad(aec_abs_sub, shuffle=FALSE)
aec_abs_shuffle <-1:1000 %>% map_dbl(~ mad(aec_abs_sub))
aec_abs_shuffle <- data.frame(d=aec_abs_shuffle, y=1:1000)
ggplot(data=aec_abs_shuffle, aes(x=d)) + geom_dotplot(binwidth=100) +
  geom_vline(xintercept=aec_abs_meandif, colour="red")
```

---

\centerline{\includegraphics[width=12in]{eechidna.pdf}}

## Pedestrians in Melbourne city

```{r eval=FALSE, echo = FALSE, message = FALSE, warning = FALSE}
# Get pedestrian sensor locations
ped_loc <- read_csv("Pedestrian_Sensor_Locations.csv")

melb <- get_map(location=c(mean(range(ped_loc$Longitude)),
                           mean(range(ped_loc$Latitude))), zoom=10)
ggmap(melb) + geom_point(data=ped_loc, 
                         aes(x=Longitude, y=Latitude), 
                         colour="#c51b7d", alpha=0.5, size=3)
```

\centerline{\includegraphics[width=4in]{sensor_locations.pdf}}

---

```{r eval=FALSE, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center'}
library(jsonlite)
limit <- 1453000 # all the up-to-date records need to be retrieved
web_add <- "https://data.melbourne.vic.gov.au/resource/mxb8-wn4w.json?"
ped_url <- paste0(web_add, "$limit=", limit)
pedestrian <- fromJSON(ped_url) # without api token
pedestrian <- tbl_df(pedestrian)
colnames(pedestrian) <- c("date_time", "day", "id", "mdate", "month", "count", "sensor_id", "sensor_name", "time", "year")
pedestrian <- pedestrian %>%
  mutate(date = as.Date(paste(pedestrian$mdate,
                              pedestrian$month,
                              pedestrian$year, sep="-"),
                        "%d-%b-%Y", tz = "AEST"),
         count = as.integer(count), sensor_id = factor(sensor_id))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Read sensor counts
ped_sub <- read_csv("pedestrian_counts_sub.csv")
ped_sub <- ped_sub %>% 
  filter(year == 2015, month == "February") %>%
  filter(sensor_name %in% c("Flinders Street Station Underpass", 
                            "Flagstaff Station", "Melbourne Central")) %>%
  dplyr::arrange(sensor_id, date, time) 
ped_sub$day <- factor(ped_sub$day, levels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
ggplot(ped_sub, aes(x=time, y=count, colour=sensor_name)) +
  facet_grid(sensor_name~day) +
  scale_colour_brewer(palette="Dark2") +
  geom_line(aes(group=date)) +
  theme(legend.position="None")
```

## Big Data day for high schoolers

- Anomalous events
- High pedestrian traffic and potential for disasters
- Grand final day effects
- Weekend/week day train station patterns
- Shopping districts

## Power to the people

With a laptop loaded with R, and access to open data, the world is your oyster!

- With knowledge of randomisation (bootstrap, permutation, simulation)
- Ability to make data plots (grammar of graphics, appropriate mapping of variables to graphical elements)
- We can learn a lot about our world

Slides and code at [https://github.com/dicook/Auckland_Teachers_Day_Nov16](https://github.com/dicook/Auckland_Teachers_Day_Nov16).

## Share and share alike

This work is licensed under the Creative Commons Attribution-Noncommercial 3.0 United States License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/ 3.0/us/ or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.
